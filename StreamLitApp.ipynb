{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUi6bEVcEw6f"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UchuFPe0Ia2E",
        "outputId": "277dc919-859c-4f3d-c6bf-2c7bc469bd60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-4iiw6_1i\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-4iiw6_1i\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=bab3e958059b4ead6fde3d29ae54ba4ebe8f206e6fe945e1e9b4e477066303e0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1l5c9bk9/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20231117 tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vurb-6g4IgAL",
        "outputId": "efa7d378-a10d-4803-84d4-59bdeca18062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-hnurk03m\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-hnurk03m\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=b115e6593406d1acd281d442dccbb1e6fcfc37070812806d807296396b2e68c1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9jsb473n/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "  Attempting uninstall: openai-whisper\n",
            "    Found existing installation: openai-whisper 20231117\n",
            "    Uninstalling openai-whisper-20231117:\n",
            "      Successfully uninstalled openai-whisper-20231117\n",
            "Successfully installed openai-whisper-20231117\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ON5YFjkenPI",
        "outputId": "0a5a6a13-208d-46bf-8f27-6c88f8f4619d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soundfile sounddevice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He6i0YyF6Lvn",
        "outputId": "c38b50e4-1036-4159-bb0f-5d81bbc33c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: sounddevice in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn1KfrGSEz1V",
        "outputId": "74fda177-9557-4627-e75b-14fc808fe5ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import whisper\n",
        "import pickle\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Bidirectional, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import soundfile as sf\n",
        "\n",
        "st.title(\"Voice authentication ğŸµ:\")\n",
        "\n",
        "audio_file = st.file_uploader(\"Upload Audio\", type=[\"wav\",\"mp3\",\"flac\"])\n",
        "st.write(type(audio_file))\n",
        "\n",
        "st.sidebar.header(\"Play the audio\")\n",
        "st.sidebar.audio(audio_file)\n",
        "\n",
        "@st.cache_data\n",
        "def load_model():\n",
        "    try:\n",
        "        with open(\"drive/MyDrive/KAGGLE/deepFake.pkl\", \"rb\") as pickle_in:\n",
        "            model = pickle.load(pickle_in)\n",
        "        return model\n",
        "    except FileNotFoundError:\n",
        "        st.sidebar.error(\"Model file not found. Please make sure the file exists.\")\n",
        "        return None\n",
        "\n",
        "# Function to extract features from an audio file\n",
        "def extract_features(file_path, segment_length, file_name):\n",
        "    try:\n",
        "        # Load the audio file\n",
        "        y, sr = librosa.load(file_path)\n",
        "        print(\"sr: \"+ str(sr))\n",
        "        # Calculate the number of segments based on the segment length and audio length\n",
        "        num_segments = int(np.ceil(len(y) / float(segment_length * sr)))\n",
        "        print(\"num_segments: \"+ str(num_segments))\n",
        "        # Initialize a list to store the features for this file\n",
        "        features = []\n",
        "\n",
        "        # Extract features for each segment\n",
        "        for i in range(num_segments):\n",
        "            # Calculate start and end frame for the current segment\n",
        "            start_frame = i * segment_length * sr\n",
        "            end_frame = min(len(y), (i + 1) * segment_length * sr)\n",
        "            # Extract audio for this segment\n",
        "            y_segment = y[start_frame:end_frame]\n",
        "\n",
        "            # Extract features\n",
        "            chroma_stft = np.mean(librosa.feature.chroma_stft(y=y_segment, sr=sr))\n",
        "            rms = np.mean(librosa.feature.rms(y=y_segment))\n",
        "            spec_cent = np.mean(librosa.feature.spectral_centroid(y=y_segment, sr=sr))\n",
        "            spec_bw = np.mean(librosa.feature.spectral_bandwidth(y=y_segment, sr=sr))\n",
        "            rolloff = np.mean(librosa.feature.spectral_rolloff(y=y_segment, sr=sr))\n",
        "            zcr = np.mean(librosa.feature.zero_crossing_rate(y_segment))\n",
        "            mfccs = librosa.feature.mfcc(y=y_segment, sr=sr)\n",
        "            mfccs_mean = np.mean(mfccs, axis=1)\n",
        "\n",
        "            # Append the extracted features to the list\n",
        "            features.append([chroma_stft, rms, spec_cent, spec_bw, rolloff, zcr, *mfccs_mean, file_name])\n",
        "\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "#Function to create dataset\n",
        "def create_dataset_test(audio_file, segment_length):  # Modify to accept audio_file\n",
        "\n",
        "    # Extract the file name without the extension\n",
        "    file_name = os.path.splitext(audio_file.name)[0]  # Use audio_file.name for path\n",
        "\n",
        "    # Extract features for the current file\n",
        "    file_features = extract_features(audio_file.name, segment_length, file_name)\n",
        "    if file_features:\n",
        "        # Append features of all segments along with the label to the dataset\n",
        "        for segment_features in file_features:\n",
        "            feature_list.append(segment_features)\n",
        "\n",
        "    # Create a DataFrame with the dataset\n",
        "    df = pd.DataFrame(feature_list, columns=['chroma_stft', 'rms', 'spectral_centroid', 'spectral_bandwidth', 'rolloff', 'zero_crossing_rate', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15', 'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20', 'origin_sample'])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def generate_csv(audio_file):\n",
        "\n",
        "  # Length of the audio segments in seconds\n",
        "  segment_length = 1  # for example, 1 second\n",
        "\n",
        "  # Create the dataset\n",
        "  dataset = create_dataset_test(audio_file.name, segment_length)\n",
        "\n",
        "  # Save the dataset to a CSV file\n",
        "  csv_output_path = 'audio.csv'\n",
        "  dataset.to_csv(csv_output_path, index=False)\n",
        "\n",
        "  print(f'Dataset created and saved to {csv_output_path}')\n",
        "  return csv_output_path\n",
        "\n",
        "def scale():\n",
        "  # Load the dataset\n",
        "  csv_output_path = 'drive/MyDrive/KAGGLE/audios.csv'\n",
        "  dataset = pd.read_csv(csv_output_path)\n",
        "\n",
        "  X = dataset.iloc[:, :-2].values  # Exclude 'origin_sample' and 'LABEL'\n",
        "  y = dataset['LABEL'].values\n",
        "\n",
        "  # Split the dataset into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Scale the features\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  scaler = MinMaxScaler()\n",
        "  X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
        "  return scaler\n",
        "\n",
        "# Function to record audio\n",
        "def record_audio():\n",
        "    st.write(\"Click the 'Record' button to start recording.\")\n",
        "    record_button = st.button(\"Record\")\n",
        "\n",
        "    if record_button:\n",
        "        st.write(\"Recording... Click the 'Stop' button to stop.\")\n",
        "        audio_data = st.record(key=\"user_recording\", format=\"wav\", sample_rate=44100)\n",
        "        st.write(\"Recording complete!\")\n",
        "        return audio_data\n",
        "\n",
        "audio_file = record_audio()\n",
        "\n",
        "if audio_file is not None:\n",
        "    st.audio(audio_file, format=\"audio/wav\", start_time=0)\n",
        "    st.sidebar.success(\"Audio recorded!\")\n",
        "\n",
        "if st.sidebar.button(\"Load the model\"):\n",
        "  model = load_model()\n",
        "  st.sidebar.success(\"Model loaded!\")\n",
        "\n",
        "if st.sidebar.button(\"Test Audio\"):\n",
        "  if audio_file is not None:\n",
        "    st.sidebar.success(\"Processing audio...\")\n",
        "    csv_file = generate_csv(audio_file)\n",
        "\n",
        "    # Use the model to make predictions on the dataset\n",
        "    st.write(\"Making predictions with the model...\")\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = pd.read_csv(csv_file)\n",
        "\n",
        "    # Encode the labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    dataset['LABEL'] = label_encoder.fit_transform(dataset['LABEL'])\n",
        "\n",
        "    X_test = dataset.iloc[:, :-2].values  # Exclude 'origin_sample'\n",
        "\n",
        "    scaler = scale()\n",
        "    X_test_scaled = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
        "    X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
        "\n",
        "    # Reshape the features to match the input shape expected by an RNN\n",
        "    X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred_binary = np.round(y_pred)\n",
        "\n",
        "    # Check if there are more 0s than 1s\n",
        "    if np.sum(y_pred_binary == 0) > np.sum(y_pred_binary == 1):\n",
        "      st.write(\"Prediction: fake\")\n",
        "    else:\n",
        "      st.write(\"Prediction: real\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukboCngRFpoH",
        "outputId": "128c480e-2aa4-4c46-8b99-653e1c1065a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.29.96.199\n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLvhEVW6E6II",
        "outputId": "d3ba89ee-103f-41fc-cca3-9096a6a2b2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.029s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DK8joaBE9Ot"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15rBWVT5FCAA",
        "outputId": "28c5002d-9b70-4883-d23a-8903addb1fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.11s\n",
            "your url is: https://rare-heads-shop.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwf5our2FFUG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}